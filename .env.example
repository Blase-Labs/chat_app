# Model/Embeddings (defaults work if using Ollama)
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://host.docker.internal:11434
LLM_MODEL=gemma3:1b
EMBEDDING_MODEL=nomic-embed-text

# Retrieval tuning (optional)
RETRIEVAL_MODE=mmr
FETCH_K=24
MMR_LAMBDA=0.7
PER_DOC_CHARS=600
MAX_CTX_CHARS=2400
MAX_ROWS=5000

# UI/Streamlit (Windows stability)
STREAMLIT_SERVER_FILE_WATCHER_TYPE=poll